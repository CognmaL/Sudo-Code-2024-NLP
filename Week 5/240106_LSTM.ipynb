{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5236465,"sourceType":"datasetVersion","datasetId":3046745},{"sourceId":9616042,"sourceType":"datasetVersion","datasetId":5868347}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align='center'><font size=\"10\" color=\"blue\"> TEXT GENERATION BY LSTM </font></div>\n\n240108 - Nguyễn Lưu Phương Ngọc Lam \n\nKaggle data path: <a> https://www.kaggle.com/datasets/iambestfeeder/10000-vietnamese-books/versions/1 </a>\n\nAims: Build a LSTM neural network for Text Generation task.\n- This code to concat all raw data of 10 Topics newspaper from `txt` file into a `dataframe` and preprocessed by dataset of `hugging face`\n- Normalize data and tokenize by PhoBERT Tranformers\n- Build model with performance in ...","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers -q\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport random\nimport pickle\nfrom string import punctuation\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-14T09:13:11.657981Z","iopub.execute_input":"2024-10-14T09:13:11.658333Z","iopub.status.idle":"2024-10-14T09:13:23.769025Z","shell.execute_reply.started":"2024-10-14T09:13:11.658300Z","shell.execute_reply":"2024-10-14T09:13:23.768036Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## 1. Load data from kaggle path","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/10000-vietnamese-books/output'\n\n# Initialize lists to store file names and their contents\ndata = []\nname = []\n\n# Load files: 137 seconds\nfor filename in os.listdir(file_path):\n    name.append(filename.split('.')[0])  # Extract the filename without extension\n    filepath = os.path.join(file_path, filename)\n    \n    # Open and read the content of the file\n    with open(filepath, 'r', encoding='utf-8') as f:\n        data.append(f.read())\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'name': name,\n    'content': data\n})","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:11:51.132534Z","iopub.execute_input":"2024-10-13T17:11:51.133238Z","iopub.status.idle":"2024-10-13T17:13:17.088138Z","shell.execute_reply.started":"2024-10-13T17:11:51.133191Z","shell.execute_reply":"2024-10-13T17:13:17.087007Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Take a look for few rows of the DataFrame\ndisplay(df.sample(5))","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:13:17.090167Z","iopub.execute_input":"2024-10-13T17:13:17.090911Z","iopub.status.idle":"2024-10-13T17:13:17.144436Z","shell.execute_reply.started":"2024-10-13T17:13:17.090862Z","shell.execute_reply":"2024-10-13T17:13:17.143534Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"                                                name  \\\n9276             Anh và em sinh đôi - Phan Hồn Nhiên   \n679            Chuyện Của Vi và Tôi - Phạm Nghi Dung   \n8956   Danuble một dòng còn quyến luyến - Dương Thụy   \n10043            Cô áo hồng, cô áo tím - Lê văn thảo   \n572                     Trong Cơn Mưa - Khúc Thụy Du   \n\n                                                 content  \n9276   \\nPhan Hồn Nhiên\\nAnh và em sinh đôi\\nSinh đôi...  \n679    \\nPhạm Nghi Dung\\nChuyện Của Vi và Tôi\\nLớp 5A...  \n8956   \\nDương Thụy\\nDanuble một dòng còn quyến luyến...  \n10043  \\nLê văn thảo\\nCô áo hồng, cô áo tím\\nH\\nai ng...  \n572    \\nKhúc Thụy Du\\nTrong Cơn Mưa\\nLời tác giả\\nKí...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9276</th>\n      <td>Anh và em sinh đôi - Phan Hồn Nhiên</td>\n      <td>\\nPhan Hồn Nhiên\\nAnh và em sinh đôi\\nSinh đôi...</td>\n    </tr>\n    <tr>\n      <th>679</th>\n      <td>Chuyện Của Vi và Tôi - Phạm Nghi Dung</td>\n      <td>\\nPhạm Nghi Dung\\nChuyện Của Vi và Tôi\\nLớp 5A...</td>\n    </tr>\n    <tr>\n      <th>8956</th>\n      <td>Danuble một dòng còn quyến luyến - Dương Thụy</td>\n      <td>\\nDương Thụy\\nDanuble một dòng còn quyến luyến...</td>\n    </tr>\n    <tr>\n      <th>10043</th>\n      <td>Cô áo hồng, cô áo tím - Lê văn thảo</td>\n      <td>\\nLê văn thảo\\nCô áo hồng, cô áo tím\\nH\\nai ng...</td>\n    </tr>\n    <tr>\n      <th>572</th>\n      <td>Trong Cơn Mưa - Khúc Thụy Du</td>\n      <td>\\nKhúc Thụy Du\\nTrong Cơn Mưa\\nLời tác giả\\nKí...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 2. Preprocessing data","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\n# Chuyển đổi DataFrame thành dict để tạo dataset\ndf_dict = df.to_dict(orient='list')  \ncleaning_dataset = Dataset.from_dict(df_dict)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:13:54.722014Z","iopub.execute_input":"2024-10-13T17:13:54.723034Z","iopub.status.idle":"2024-10-13T17:14:12.689188Z","shell.execute_reply.started":"2024-10-13T17:13:54.722990Z","shell.execute_reply":"2024-10-13T17:14:12.688212Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def extract_author_and_book(examples):\n    book_names = []\n    authors = []\n    \n    for name, content in zip(examples['name'], examples['content']):\n        # Split the name to get the book name\n        parts = name.split(' - ')\n        book_name = parts[0].strip()\n        \n        # Extract the relevant part from content\n        relevant_section = content.split('\\n')[1:3]  # Get the second and third lines\n        author = relevant_section[0].strip()  # Author from the second line\n        book_name = relevant_section[1].strip()  # Book name from the third line\n\n        # Append the results\n        book_names.append(book_name)\n        authors.append(author)\n    \n    return {'book_name': book_names, 'author': authors}\n\n# Apply the extract_author_and_book function in a batched way - 31 seconds\ncleaning_dataset = cleaning_dataset.map(extract_author_and_book, batched=True)\n\n# Convert back to DataFrame\ncleaning_df = cleaning_dataset.to_pandas()\ncleaning_df = cleaning_df[['book_name', 'author', 'content']]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:16:41.069516Z","iopub.execute_input":"2024-10-13T17:16:41.070534Z","iopub.status.idle":"2024-10-13T17:17:12.943867Z","shell.execute_reply.started":"2024-10-13T17:16:41.070491Z","shell.execute_reply":"2024-10-13T17:17:12.942713Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10415 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62035751ea7441ec8380a107c1b270c1"}},"metadata":{}}]},{"cell_type":"code","source":"display(cleaning_df.sample(5))","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:18:17.466441Z","iopub.execute_input":"2024-10-13T17:18:17.467360Z","iopub.status.idle":"2024-10-13T17:18:17.478951Z","shell.execute_reply.started":"2024-10-13T17:18:17.467315Z","shell.execute_reply":"2024-10-13T17:18:17.477885Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"                       book_name          author  \\\n6146  Ông Thợ Giày Và Cô Con Gái   Cao Hành Kiện   \n3969           Chiếc Vòng Pha Lê       QUỲNH DAO   \n108            Bao giờ thì cưới?        Tường Vi   \n4391                     Lụm Còi  Nguyễn Ngọc Tư   \n6063    Một Truyện Ngắn Hay Nhất        Linh Bảo   \n\n                                                content  \n6146  \\nCao Hành Kiện\\nÔng Thợ Giày Và Cô Con Gái\\nT...  \n3969  \\nQUỲNH DAO\\nChiếc Vòng Pha Lê\\nVào lúc năm cù...  \n108   \\nTường Vi\\nBao giờ thì cưới?\\nTom và nàng yêu...  \n4391  \\nNguyễn Ngọc Tư\\nLụm Còi\\nTôi quyết định rồi,...  \n6063  \\nLinh Bảo\\nMột Truyện Ngắn Hay Nhất\\n(trích t...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_name</th>\n      <th>author</th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6146</th>\n      <td>Ông Thợ Giày Và Cô Con Gái</td>\n      <td>Cao Hành Kiện</td>\n      <td>\\nCao Hành Kiện\\nÔng Thợ Giày Và Cô Con Gái\\nT...</td>\n    </tr>\n    <tr>\n      <th>3969</th>\n      <td>Chiếc Vòng Pha Lê</td>\n      <td>QUỲNH DAO</td>\n      <td>\\nQUỲNH DAO\\nChiếc Vòng Pha Lê\\nVào lúc năm cù...</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>Bao giờ thì cưới?</td>\n      <td>Tường Vi</td>\n      <td>\\nTường Vi\\nBao giờ thì cưới?\\nTom và nàng yêu...</td>\n    </tr>\n    <tr>\n      <th>4391</th>\n      <td>Lụm Còi</td>\n      <td>Nguyễn Ngọc Tư</td>\n      <td>\\nNguyễn Ngọc Tư\\nLụm Còi\\nTôi quyết định rồi,...</td>\n    </tr>\n    <tr>\n      <th>6063</th>\n      <td>Một Truyện Ngắn Hay Nhất</td>\n      <td>Linh Bảo</td>\n      <td>\\nLinh Bảo\\nMột Truyện Ngắn Hay Nhất\\n(trích t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def normalize_text(text):    \n    # Remove content starting from \"Mục lục\"\n    index = text.find(\"Mục lục\")\n    if index != -1:\n        text = text[:index]\n    \n    # Remove HTML tags, URLs\n    text = re.sub(r'<[^>]*>', '', text)\n    text = re.sub(r'Nguồn:\\s*http?:\\/\\/\\S+', '', text)\n    text = re.sub(r'http\\S*', '', text)\n    \n    # Remove punctuation, digits, and special symbols\n    text = re.sub(f'[{punctuation}₫—℅\\d\\n\\t]', ' ', text)\n    \n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove extra whitespaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndef clean_content(text):\n    results = []\n    for book_name, author, content in zip(text['book_name'], text['author'], text['content']):\n        # Loại bỏ book_name và author khỏi content\n        cleaned_content = re.sub(re.escape(book_name), '', content, flags=re.IGNORECASE)\n        cleaned_content = re.sub(re.escape(author), '', cleaned_content, flags=re.IGNORECASE)\n        \n        # Chuẩn hóa nội dung\n        cleaned_content = normalize_text(cleaned_content)\n        results.append(cleaned_content)\n    return {'cleaned_content': results}\n\ncleaning_dataset = cleaning_dataset.map(clean_content, batched=True) # 5 minutes\n\n# Chuyển đổi thành df để display\ncleaning_df = cleaning_dataset.to_pandas()\ndisplay(cleaning_df.sample(5))","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:18:30.967316Z","iopub.execute_input":"2024-10-13T17:18:30.967754Z","iopub.status.idle":"2024-10-13T17:23:49.335488Z","shell.execute_reply.started":"2024-10-13T17:18:30.967715Z","shell.execute_reply":"2024-10-13T17:23:49.334319Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10415 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f58452d96549b5b1b898cfba53163c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                        name  \\\n4659        Ngày tháng nào - Tôn Nữ Thu Dung   \n5297  Đỗ nương nương báo oán - Hồ Biểu Chánh   \n3712           Dòng sông thơ ấu - Hồ Huy Sơn   \n7892            Chim khách kêu - Nguyễn Kiên   \n8685                Bài toán - Vũ Thư Nguyên   \n\n                                                content  \\\n4659  \\nTôn Nữ Thu Dung\\nNgày tháng nào\\nChương 1\\nE...   \n5297  \\nHồ Biểu Chánh\\nĐỗ nương nương báo oán\\nChươn...   \n3712  \\nHồ Huy Sơn\\nDòng sông thơ ấu\\n1. Bố mẹ tôi l...   \n7892  \\nNguyễn Kiên\\nChim khách kêu\\nBuổi sáng có co...   \n8685  \\nVũ Thư Nguyên\\nBài toán\\nVăn đẩy tấm cửa kiế...   \n\n                   book_name           author  \\\n4659          Ngày tháng nào  Tôn Nữ Thu Dung   \n5297  Đỗ nương nương báo oán    Hồ Biểu Chánh   \n3712        Dòng sông thơ ấu       Hồ Huy Sơn   \n7892          Chim khách kêu      Nguyễn Kiên   \n8685                Bài toán    Vũ Thư Nguyên   \n\n                                        cleaned_content  \n4659  chương em khoan khoái nhìn căn phòng vừa được ...  \n5297  chương bĩ thới tuần huờn trái đất vần xây tối ...  \n3712  bố mẹ tôi ly dị bên nội không ai chịu nhận tôi...  \n7892  buổi sáng có con chim khách đến kêu trước cửa ...  \n8685  văn đẩy tấm cửa kiếng lách mình ra ngoài lan c...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>content</th>\n      <th>book_name</th>\n      <th>author</th>\n      <th>cleaned_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4659</th>\n      <td>Ngày tháng nào - Tôn Nữ Thu Dung</td>\n      <td>\\nTôn Nữ Thu Dung\\nNgày tháng nào\\nChương 1\\nE...</td>\n      <td>Ngày tháng nào</td>\n      <td>Tôn Nữ Thu Dung</td>\n      <td>chương em khoan khoái nhìn căn phòng vừa được ...</td>\n    </tr>\n    <tr>\n      <th>5297</th>\n      <td>Đỗ nương nương báo oán - Hồ Biểu Chánh</td>\n      <td>\\nHồ Biểu Chánh\\nĐỗ nương nương báo oán\\nChươn...</td>\n      <td>Đỗ nương nương báo oán</td>\n      <td>Hồ Biểu Chánh</td>\n      <td>chương bĩ thới tuần huờn trái đất vần xây tối ...</td>\n    </tr>\n    <tr>\n      <th>3712</th>\n      <td>Dòng sông thơ ấu - Hồ Huy Sơn</td>\n      <td>\\nHồ Huy Sơn\\nDòng sông thơ ấu\\n1. Bố mẹ tôi l...</td>\n      <td>Dòng sông thơ ấu</td>\n      <td>Hồ Huy Sơn</td>\n      <td>bố mẹ tôi ly dị bên nội không ai chịu nhận tôi...</td>\n    </tr>\n    <tr>\n      <th>7892</th>\n      <td>Chim khách kêu - Nguyễn Kiên</td>\n      <td>\\nNguyễn Kiên\\nChim khách kêu\\nBuổi sáng có co...</td>\n      <td>Chim khách kêu</td>\n      <td>Nguyễn Kiên</td>\n      <td>buổi sáng có con chim khách đến kêu trước cửa ...</td>\n    </tr>\n    <tr>\n      <th>8685</th>\n      <td>Bài toán - Vũ Thư Nguyên</td>\n      <td>\\nVũ Thư Nguyên\\nBài toán\\nVăn đẩy tấm cửa kiế...</td>\n      <td>Bài toán</td>\n      <td>Vũ Thư Nguyên</td>\n      <td>văn đẩy tấm cửa kiếng lách mình ra ngoài lan c...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"cleaning_df = cleaning_df[['book_name', 'author', 'cleaned_content']]\ndisplay(cleaning_df.sample(5))\n\noutput_file_path = '/kaggle/working/cleaning_data.csv'\ncleaning_df.to_csv(output_file_path, index = False)\nprint(f\"File saved to {output_file_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:24:43.242952Z","iopub.execute_input":"2024-10-13T17:24:43.243794Z","iopub.status.idle":"2024-10-13T17:25:42.244586Z","shell.execute_reply.started":"2024-10-13T17:24:43.243747Z","shell.execute_reply":"2024-10-13T17:25:42.243501Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"                             book_name                   author  \\\n2947            Nói không với tiêu cực             Bùi Đức Hiền   \n8100                            Ven Hồ          Lê Thị Thu Thủy   \n7720                Bảy ngày trong đời       Nguyễn Thị Thu Huệ   \n3583  Điện Biên Phủ - Điểm hẹn lịch sử           Võ Nguyên Giáp   \n1779                  Nữ Chúa Hồ Ba Bể  Hoàng Ly - Đỗ Hồng Linh   \n\n                                        cleaned_content  \n2947  sáu giờ chiều trời sẩm tối đèn đã bật sáng tro...  \n8100  bây giờ mọi cái không còn đủ sức làm tôi phải ...  \n7720  mai anh đi mấy giờ lụa hỏi bốn giờ sáng bé tí ...  \n3583  chương cuộc họp ở tỉn keo cuộc kháng chiế...  \n1779  phần thứ nhất chương loạn rừng thượng du đất v...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_name</th>\n      <th>author</th>\n      <th>cleaned_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2947</th>\n      <td>Nói không với tiêu cực</td>\n      <td>Bùi Đức Hiền</td>\n      <td>sáu giờ chiều trời sẩm tối đèn đã bật sáng tro...</td>\n    </tr>\n    <tr>\n      <th>8100</th>\n      <td>Ven Hồ</td>\n      <td>Lê Thị Thu Thủy</td>\n      <td>bây giờ mọi cái không còn đủ sức làm tôi phải ...</td>\n    </tr>\n    <tr>\n      <th>7720</th>\n      <td>Bảy ngày trong đời</td>\n      <td>Nguyễn Thị Thu Huệ</td>\n      <td>mai anh đi mấy giờ lụa hỏi bốn giờ sáng bé tí ...</td>\n    </tr>\n    <tr>\n      <th>3583</th>\n      <td>Điện Biên Phủ - Điểm hẹn lịch sử</td>\n      <td>Võ Nguyên Giáp</td>\n      <td>chương cuộc họp ở tỉn keo cuộc kháng chiế...</td>\n    </tr>\n    <tr>\n      <th>1779</th>\n      <td>Nữ Chúa Hồ Ba Bể</td>\n      <td>Hoàng Ly - Đỗ Hồng Linh</td>\n      <td>phần thứ nhất chương loạn rừng thượng du đất v...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"File saved to /kaggle/working/cleaning_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Group by author and aggregate the count of books\nbook_count = df.groupby('author').size().reset_index(name='number_of_books')\n\nprint(book_count)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download file from Kaggle\nfrom IPython.display import FileLink\nFileLink('cleaning_data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-11T17:37:26.244121Z","iopub.execute_input":"2024-10-11T17:37:26.244528Z","iopub.status.idle":"2024-10-11T17:37:26.251137Z","shell.execute_reply.started":"2024-10-11T17:37:26.244490Z","shell.execute_reply":"2024-10-11T17:37:26.250083Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/cleaning_data.csv","text/html":"<a href='cleaning_data.csv' target='_blank'>cleaning_data.csv</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"> The processing of the DataFrame as described above aims to enhance classification tasks if applicable.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n\n# Tokenize the content data - 26 minutes\ndef tokenize_function(text):\n    return tokenizer(text['cleaned_content'], padding=True, truncation=True, max_length=512)\n\ncleaned_dataset = cleaning_dataset.map(tokenize_function, batched=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_file_path = '/kaggle/working/cleaned_data.csv'\ncleaned_df = cleaned_dataset.to_pandas()\ncleaned_df.to_csv(output_file_path, index = False)\nprint(f\"File saved to {output_file_path}\")\n\nFileLink('cleaned_data.csv') # Download file from Kaggle","metadata":{"execution":{"iopub.status.busy":"2024-10-11T18:09:24.047654Z","iopub.execute_input":"2024-10-11T18:09:24.048061Z","iopub.status.idle":"2024-10-11T18:12:31.866860Z","shell.execute_reply.started":"2024-10-11T18:09:24.048023Z","shell.execute_reply":"2024-10-11T18:12:31.865803Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"File saved to /kaggle/working/cleaned_data.csv\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/cleaned_data.csv","text/html":"<a href='cleaned_data.csv' target='_blank'>cleaned_data.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate text length (number of characters)\ncleaned_df['text_length'] = cleaned_df['cleaned_content'].apply(len)\ncleaned_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-11T18:19:40.929477Z","iopub.execute_input":"2024-10-11T18:19:40.930403Z","iopub.status.idle":"2024-10-11T18:19:40.976304Z","shell.execute_reply.started":"2024-10-11T18:19:40.930350Z","shell.execute_reply":"2024-10-11T18:19:40.975299Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                                  name  \\\n0               Nhà ảo thuật - Mạc Can   \n1                Ra đi - Trần Chi Liên   \n2             Đường về - Vũ Thư Nguyên   \n3              Bốn Thằng Buồn - Phi Va   \n4  Chiếc Lexus và cây Ô liu - Thomas L   \n\n                                             content  \\\n0  \\nMạc Can\\nNhà ảo thuật\\nCó một cậu bé muốn họ...   \n1  \\nTrần Chi Liên\\nRa đi\\nTặng người đồng cảnh t...   \n2  \\nVũ Thư Nguyên\\nĐường về\\nVừa ra khỏi xa lộ 6...   \n3  \\nPhi Va\\nBốn Thằng Buồn\\n&quot;Bốn người lính...   \n4  \\nThomas L. Friedman\\nChiếc Lexus và cây Ô liu...   \n\n                  book_name         author  \\\n0              Nhà ảo thuật        Mạc Can   \n1                     Ra đi  Trần Chi Liên   \n2                  Đường về  Vũ Thư Nguyên   \n3            Bốn Thằng Buồn         Phi Va   \n4  Chiếc Lexus và cây Ô liu       Thomas L   \n\n                                     cleaned_content  \\\n0  có một cậu bé muốn học vài trò ảo thuật nhưng ...   \n1  tặng người đồng cảnh tương lân thế rồi những n...   \n2  vừa ra khỏi xa lộ kenneth tăng tốc độ rẽ vào x...   \n3  quot bốn người lính buồn vẫn đó trầm lặng nghĩ...   \n4  friedman lời mở đầu đây là ấn bản bìa mềm cuốn...   \n\n                                           input_ids  \\\n0  [0, 10, 16, 881, 308, 202, 222, 515, 2680, 215...   \n1  [0, 806, 18, 80, 805, 8840, 14481, 570, 182, 2...   \n2  [0, 164, 40, 353, 604, 1776, 1493, 13564, 2015...   \n3  [0, 2845, 12440, 1586, 18, 2471, 1520, 74, 37,...   \n4  [0, 17869, 3995, 3206, 278, 548, 127, 97, 8, 4...   \n\n                                      token_type_ids  \\\n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                      attention_mask  text_length  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...         9901  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...        12375  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...        20383  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...         3098  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...      1120791  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>content</th>\n      <th>book_name</th>\n      <th>author</th>\n      <th>cleaned_content</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n      <th>text_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Nhà ảo thuật - Mạc Can</td>\n      <td>\\nMạc Can\\nNhà ảo thuật\\nCó một cậu bé muốn họ...</td>\n      <td>Nhà ảo thuật</td>\n      <td>Mạc Can</td>\n      <td>có một cậu bé muốn học vài trò ảo thuật nhưng ...</td>\n      <td>[0, 10, 16, 881, 308, 202, 222, 515, 2680, 215...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>9901</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ra đi - Trần Chi Liên</td>\n      <td>\\nTrần Chi Liên\\nRa đi\\nTặng người đồng cảnh t...</td>\n      <td>Ra đi</td>\n      <td>Trần Chi Liên</td>\n      <td>tặng người đồng cảnh tương lân thế rồi những n...</td>\n      <td>[0, 806, 18, 80, 805, 8840, 14481, 570, 182, 2...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>12375</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Đường về - Vũ Thư Nguyên</td>\n      <td>\\nVũ Thư Nguyên\\nĐường về\\nVừa ra khỏi xa lộ 6...</td>\n      <td>Đường về</td>\n      <td>Vũ Thư Nguyên</td>\n      <td>vừa ra khỏi xa lộ kenneth tăng tốc độ rẽ vào x...</td>\n      <td>[0, 164, 40, 353, 604, 1776, 1493, 13564, 2015...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>20383</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bốn Thằng Buồn - Phi Va</td>\n      <td>\\nPhi Va\\nBốn Thằng Buồn\\n&amp;quot;Bốn người lính...</td>\n      <td>Bốn Thằng Buồn</td>\n      <td>Phi Va</td>\n      <td>quot bốn người lính buồn vẫn đó trầm lặng nghĩ...</td>\n      <td>[0, 2845, 12440, 1586, 18, 2471, 1520, 74, 37,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>3098</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Chiếc Lexus và cây Ô liu - Thomas L</td>\n      <td>\\nThomas L. Friedman\\nChiếc Lexus và cây Ô liu...</td>\n      <td>Chiếc Lexus và cây Ô liu</td>\n      <td>Thomas L</td>\n      <td>friedman lời mở đầu đây là ấn bản bìa mềm cuốn...</td>\n      <td>[0, 17869, 3995, 3206, 278, 548, 127, 97, 8, 4...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>1120791</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"columns_to_keep = ['book_name', 'author', 'cleaned_content', 'input_ids', 'token_type_ids', 'attention_mask']\ncleaned_dataset = cleaned_dataset.select_columns(columns_to_keep)","metadata":{"execution":{"iopub.status.busy":"2024-10-11T18:23:22.472698Z","iopub.execute_input":"2024-10-11T18:23:22.473672Z","iopub.status.idle":"2024-10-11T18:23:22.485281Z","shell.execute_reply.started":"2024-10-11T18:23:22.473630Z","shell.execute_reply":"2024-10-11T18:23:22.484527Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"cleaned_dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-11T18:23:26.886964Z","iopub.execute_input":"2024-10-11T18:23:26.887884Z","iopub.status.idle":"2024-10-11T18:23:26.893815Z","shell.execute_reply.started":"2024-10-11T18:23:26.887839Z","shell.execute_reply":"2024-10-11T18:23:26.892820Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['book_name', 'author', 'cleaned_content', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 10415\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## 3. Split the dataset to train - test set","metadata":{}},{"cell_type":"code","source":"# Load data if the session crash\npath = '/kaggle/input/data-ids/data_ids.xlsx'\ndf = pd.read_excel(path)\ndf['input_ids'] = df['input_ids'].apply(lambda x: [int(i) for i in x.strip('[]').split()])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T08:08:27.823320Z","iopub.execute_input":"2024-10-14T08:08:27.823722Z","iopub.status.idle":"2024-10-14T08:08:30.456865Z","shell.execute_reply.started":"2024-10-14T08:08:27.823686Z","shell.execute_reply":"2024-10-14T08:08:30.455913Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                           input_ids\n0  [0, 10, 16, 881, 308, 202, 222, 515, 2680, 215...\n1  [0, 806, 18, 80, 805, 8840, 14481, 570, 182, 2...\n2  [0, 164, 40, 353, 604, 1776, 1493, 13564, 2015...\n3  [0, 2845, 12440, 1586, 18, 2471, 1520, 74, 37,...\n4  [0, 17869, 3995, 3206, 278, 548, 127, 97, 8, 4...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[0, 10, 16, 881, 308, 202, 222, 515, 2680, 215...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[0, 806, 18, 80, 805, 8840, 14481, 570, 182, 2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0, 164, 40, 353, 604, 1776, 1493, 13564, 2015...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[0, 2845, 12440, 1586, 18, 2471, 1520, 74, 37,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[0, 17869, 3995, 3206, 278, 548, 127, 97, 8, 4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 4. Define model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom tensorflow.keras.models import load_model  # Import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, Dense\n\nclass LSTMModel:\n    def __init__(self, vocab_size, embedding_dim, lstm_units, max_length):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.lstm_units = lstm_units\n        self.max_length = max_length\n        \n        # Initialize the model\n        self.model = self.build_model()\n        \n        # Load the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n        \n    def build_model(self):\n        \"\"\"Builds and returns the LSTM model.\"\"\"\n        model = Sequential()\n        model.add(Input(shape=(self.max_length,)))  # Add Input Layer with shape\n        model.add(Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim))\n        model.add(LSTM(self.lstm_units, return_sequences=True))\n        model.add(Dropout(0.5))  # Dropout for regularization\n        model.add(LSTM(self.lstm_units))\n        model.add(Dense(self.vocab_size, activation='softmax'))  # Softmax for generative model\n        return model\n\n    def compile_model(self):\n        \"\"\"Compiles the model.\"\"\"\n        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n    def fit(self, X_train, y_train, epochs=10, batch_size=32, validation_split=0.1):\n        \"\"\"Trains the model on the training data.\"\"\"\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n        self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stopping])\n\n    def evaluate(self, X_test, y_test):\n        \"\"\"Evaluates the model on the test data.\"\"\"\n        loss, accuracy = self.model.evaluate(X_test, y_test)\n        print(f'|Test Loss: {loss}| --------------------- |Test Accuracy: {accuracy}|')\n\n    def save_model(self, filepath):\n        \"\"\"Saves the model to the specified filepath.\"\"\"\n        self.model.save(filepath)\n        print(f'Model saved to {filepath}')\n\n    @classmethod\n    def load_model(cls, filepath):\n        \"\"\"Loads a model from the specified filepath.\"\"\"\n        model = load_model(filepath)  # Use load_model from keras\n        instance = cls(vocab_size=10000, embedding_dim=128, lstm_units=64, max_length=50)  # Use the parameters you defined\n        instance.model = model\n        instance.tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")  # Reload the tokenizer\n        return instance\n\n    def generate_text(self, seed_text, gen_length):\n        \"\"\"Generates text given a seed input and desired length.\"\"\"\n        generated_text = seed_text\n        for _ in range(gen_length):\n            # Convert seed text to a sequence\n            input_sequence = [self.text_to_sequence(generated_text)]\n            input_sequence = pad_sequences(input_sequence, maxlen=self.max_length, padding='post')\n\n            # Get the predicted next word\n            predicted_probs = self.model.predict(input_sequence, verbose=0)\n            predicted_word_index = np.argmax(predicted_probs, axis=-1)[0]\n\n            # Convert the predicted index back to a word\n            next_word = self.sequence_to_text(predicted_word_index)\n\n            # Update generated_text with the new word\n            generated_text += \" \" + next_word\n        return generated_text\n\n    def text_to_sequence(self, text):\n        \"\"\"Converts text to a sequence of integers based on the vocabulary.\"\"\"\n        return self.tokenizer.encode(text, add_special_tokens=False)\n\n    def sequence_to_text(self, index):\n        \"\"\"Converts an index back to text.\"\"\"\n        return self.tokenizer.decode([index], skip_special_tokens=True)  # Convert index to list for decoding\n\ndef prepare_data_from_sequences(input_sequences, total_words):\n    \"\"\"\n    Convert input sequences into predictors and labels, similar to a language model task.\n    Each sequence will be used to predict the next word in the sequence.\n    \"\"\"\n    # Calculate the maximum sequence length\n    max_sequence_len = max([len(x) for x in input_sequences])\n    \n    # Pad sequences to have uniform length\n    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n    \n    # Split sequences into predictors and labels\n    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n    \n    # Convert labels to categorical (one-hot encoding)\n    label = to_categorical(label, num_classes=total_words)\n    \n    return predictors, label, max_sequence_len\n\n# Usage example\nif __name__ == \"__main__\":\n    path = '/kaggle/input/data-ids/data_ids.xlsx'\n    df = pd.read_excel(path)\n    df['input_ids'] = df['input_ids'].apply(lambda x: [int(i) for i in x.strip('[]').split()])    \n    \n    # Set the total number of unique tokens based on your tokenizer\n    total_words = 10000  # Adjust this as necessary\n\n    # Prepare predictors and labels\n    predictors, labels, max_sequence_len = prepare_data_from_sequences(df['input_ids'].tolist(), total_words)\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(predictors, labels, test_size=0.2, random_state=42)\n\n    # Set parameters for the LSTM model\n    vocab_size = total_words\n    embedding_dim = 128\n    lstm_units = 64\n\n    # Create an instance of LSTMModel\n    lstm_model = LSTMModel(vocab_size, embedding_dim, lstm_units, max_sequence_len)\n\n    # Compile the model\n    lstm_model.compile_model()\n\n    # Print model summary\n    lstm_model.model.summary()\n\n    # Train the model\n    lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n\n    # Save the model\n    lstm_model.save_model(\"lstm_text_generator.h5\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T08:31:22.394581Z","iopub.execute_input":"2024-10-14T08:31:22.395000Z","iopub.status.idle":"2024-10-14T08:33:26.277880Z","shell.execute_reply.started":"2024-10-14T08:31:22.394965Z","shell.execute_reply":"2024-10-14T08:33:26.276878Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_6\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m1,280,000\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m49,408\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_13 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)          │       \u001b[38;5;34m650,000\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ lstm_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">650,000</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,012,432\u001b[0m (7.68 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,012,432</span> (7.68 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,012,432\u001b[0m (7.68 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,012,432</span> (7.68 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 51ms/step - accuracy: 0.9555 - loss: 4.0379 - val_accuracy: 0.9868 - val_loss: 0.0889\nEpoch 2/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.9755 - loss: 0.1298 - val_accuracy: 0.9868 - val_loss: 0.0801\nEpoch 3/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.9793 - loss: 0.1070 - val_accuracy: 0.9868 - val_loss: 0.0779\nEpoch 4/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.9767 - loss: 0.1141 - val_accuracy: 0.9868 - val_loss: 0.0741\nEpoch 5/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.9768 - loss: 0.1128 - val_accuracy: 0.9868 - val_loss: 0.0728\nEpoch 6/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.9763 - loss: 0.1140 - val_accuracy: 0.9868 - val_loss: 0.0886\nEpoch 7/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.9756 - loss: 0.1222 - val_accuracy: 0.9868 - val_loss: 0.0679\nEpoch 8/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.9785 - loss: 0.0684 - val_accuracy: 1.0000 - val_loss: 0.0127\nEpoch 9/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.9996 - loss: 0.0136 - val_accuracy: 1.0000 - val_loss: 0.0048\nEpoch 10/10\n\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.9998 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0030\nModel saved to lstm_text_generator.h5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5. Text generation Inference","metadata":{}},{"cell_type":"code","source":"# Load the saved model\nloaded_model = LSTMModel.load_model(\"lstm_text_generator.h5\")\n\n# Perform inference and generate text based on seed input\nseed_text = \"Hôm nay trời\"  # Example seed text\ngenerated_text = loaded_model.generate_text(seed_text, gen_length=20) \nprint(\"Generated text:\", generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T08:33:58.691023Z","iopub.execute_input":"2024-10-14T08:33:58.691963Z","iopub.status.idle":"2024-10-14T08:34:01.071515Z","shell.execute_reply.started":"2024-10-14T08:33:58.691920Z","shell.execute_reply":"2024-10-14T08:34:01.070509Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Generated text: Hôm nay trời                    \n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Chuẩn bị dữ liệu\n# Load data if the session crash\npath = '/kaggle/input/data-ids/data_ids.xlsx'\ndf = pd.read_excel(path)\ndf['input_ids'] = df['input_ids'].apply(lambda x: [int(i) for i in x.strip('[]').split()])\ninput_sequences = df['input_ids'].tolist()\n\n# Kiểm tra giá trị tối đa trong input_ids\nmax_value = max(max(seq) for seq in input_sequences)\nprint(\"Giá trị lớn nhất trong input_ids:\", max_value)\n\n# Cập nhật total_words dựa trên giá trị tối đa\ntotal_words = max_value + 1  # Thêm 1 để bao gồm giá trị lớn nhất\n\ndef prepare_data_from_sequences(input_sequences, total_words):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n    label = to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\n# Gọi hàm để chuẩn bị dữ liệu\nX, y, max_sequence_length = prepare_data_from_sequences(input_sequences, total_words)\n\n# Khởi tạo mô hình LSTM\nembedding_dim = 128  # Kích thước của vector nhúng\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=total_words, output_dim=embedding_dim, input_length=max_sequence_length))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(total_words, activation='softmax'))\n\n# Biên dịch mô hình\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Huấn luyện mô hình\nmodel.fit(X, y, batch_size=64, epochs=10, validation_split=0.2)\n\n# Hàm sinh văn bản\ndef generate_text(seed_ids, gen_length=100):\n    generated_sequence = seed_ids.copy()\n    \n    for _ in range(gen_length):\n        padded_input = pad_sequences([generated_sequence], maxlen=max_sequence_length, padding='pre')\n        predicted = model.predict(padded_input, verbose=0)\n        predicted_index = np.argmax(predicted, axis=-1)[0]\n        generated_sequence.append(predicted_index)\n\n    return generated_sequence\n\n# Sử dụng hàm generate_text để sinh văn bản\nseed_text = [0]  # Sử dụng ID ban đầu như là seed\ngenerated_ids = generate_text(seed_text, gen_length=100)\n\n# In ra kết quả\nprint(\"Generated IDs:\", generated_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T09:13:55.980983Z","iopub.execute_input":"2024-10-14T09:13:55.981658Z","iopub.status.idle":"2024-10-14T09:16:30.915678Z","shell.execute_reply.started":"2024-10-14T09:13:55.981615Z","shell.execute_reply":"2024-10-14T09:16:30.914615Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Giá trị lớn nhất trong input_ids: 63999\nEpoch 1/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 113ms/step - accuracy: 0.9377 - loss: 6.3043 - val_accuracy: 0.9779 - val_loss: 0.1291\nEpoch 2/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9790 - loss: 0.1195 - val_accuracy: 0.9779 - val_loss: 0.1150\nEpoch 3/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 103ms/step - accuracy: 0.9820 - loss: 0.0982 - val_accuracy: 0.9779 - val_loss: 0.1111\nEpoch 4/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9797 - loss: 0.1043 - val_accuracy: 0.9779 - val_loss: 0.1102\nEpoch 5/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9771 - loss: 0.1125 - val_accuracy: 0.9779 - val_loss: 0.1113\nEpoch 6/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9792 - loss: 0.1041 - val_accuracy: 0.9779 - val_loss: 0.1080\nEpoch 7/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 103ms/step - accuracy: 0.9809 - loss: 0.0968 - val_accuracy: 0.9779 - val_loss: 0.1076\nEpoch 8/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9798 - loss: 0.1007 - val_accuracy: 0.9779 - val_loss: 0.1077\nEpoch 9/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9787 - loss: 0.1047 - val_accuracy: 0.9779 - val_loss: 0.1072\nEpoch 10/10\n\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9762 - loss: 0.1145 - val_accuracy: 0.9779 - val_loss: 0.1082\nGenerated IDs: [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Kết quả như vậy cho thấy mô hình của tôi chưa học được mối quan hệ ngữ nghĩa trong dữ liệu.","metadata":{}},{"cell_type":"markdown","source":"--------------------","metadata":{}}]}