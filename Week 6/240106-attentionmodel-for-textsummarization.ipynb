{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9513260,"sourceType":"datasetVersion","datasetId":5791076}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers evaluate datasets rouge_score -q","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:21:11.986844Z","iopub.execute_input":"2024-10-19T15:21:11.987492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import essential libraries\nimport os\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom datasets import Dataset\nfrom transformers import (\n    TFAutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n)\nimport evaluate","metadata":{"execution":{"iopub.status.busy":"2024-10-19T05:01:18.765343Z","iopub.execute_input":"2024-10-19T05:01:18.765744Z","iopub.status.idle":"2024-10-19T05:01:41.469402Z","shell.execute_reply.started":"2024-10-19T05:01:18.765699Z","shell.execute_reply":"2024-10-19T05:01:41.468433Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Configuration\nnum_workers = os.cpu_count()\nepochs = 7\nlearning_rate = 2e-5\n\n# Model \nmodel_name = 'vinai/bartpho-word'  \n\n# Warnings configuration\nwarnings.filterwarnings('ignore')\n\n# Load the model and tokenizer (TensorFlow version)\nmodel = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T06:31:25.735036Z","iopub.execute_input":"2024-10-19T06:31:25.735877Z","iopub.status.idle":"2024-10-19T06:31:30.318596Z","shell.execute_reply.started":"2024-10-19T06:31:25.735835Z","shell.execute_reply":"2024-10-19T06:31:30.317617Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFMBartForConditionalGeneration.\n\nSome layers of TFMBartForConditionalGeneration were not initialized from the model checkpoint at vinai/bartpho-word and are newly initialized: ['final_logits_bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"out_dir = '/kaggle/working'  # Output directory\ntrain_path = '/kaggle/input/vietnews-dataset/train.csv'  # Path to training data\nvalid_path = '/kaggle/input/vietnews-dataset/valid.csv'  # Path to validation data\ntest_path = '/kaggle/input/vietnews-dataset/test.csv'  # Path to test data\n\n# Convert datasets to Hugging Face format after cleaning\ntrain_df[\"Content\"] = train_df[\"Content\"].astype(str)\nvalid_df[\"Content\"] = valid_df[\"Content\"].astype(str)\ntest_df[\"Content\"] = test_df[\"Content\"].astype(str)\ntrain_df[\"Abstract\"] = train_df[\"Abstract\"].astype(str)\nvalid_df[\"Abstract\"] = valid_df[\"Abstract\"].astype(str)\ntest_df[\"Abstract\"] = test_df[\"Abstract\"].astype(str)\n\ntrain_ds = Dataset.from_pandas(train_df)\nvalid_ds = Dataset.from_pandas(valid_df)\ntest_ds = Dataset.from_pandas(test_df)\n\n# Preprocess function to tokenize Vietnamese inputs and outputs\ndef preprocessing(examples):\n    \"\"\"\n    Tokenizes the input Vietnamese text (Content) and prepares the model inputs for training.\n    Uses the tokenizer's maximum length to handle longer documents.\n    \"\"\"\n    inputs = [doc for doc in examples[\"Content\"]]\n    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n    \n    # Tokenizing Abstracts (targets)\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"Abstract\"], max_length=1024, truncation=True, padding=\"max_length\")\n    \n    # Adding labels to the inputs\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Tokenizing the Vietnamese datasets\ntokenized_train = train_ds.map(preprocessing, batched=True)\ntokenized_valid = valid_ds.map(preprocessing, batched=True)\ntokenized_test = test_ds.map(preprocessing, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T05:38:03.563142Z","iopub.execute_input":"2024-10-19T05:38:03.563781Z","iopub.status.idle":"2024-10-19T05:48:33.871683Z","shell.execute_reply.started":"2024-10-19T05:38:03.563743Z","shell.execute_reply":"2024-10-19T05:48:33.870730Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/105418 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01828f6f057346c49fb2f298454b6321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22642 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2acbb0e7f98841a6956d36dfdf9abfea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22644 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6850de2ac13048afa467e196e35cfa91"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_train","metadata":{"execution":{"iopub.status.busy":"2024-10-19T06:33:52.481060Z","iopub.execute_input":"2024-10-19T06:33:52.481574Z","iopub.status.idle":"2024-10-19T06:33:52.488305Z","shell.execute_reply.started":"2024-10-19T06:33:52.481523Z","shell.execute_reply":"2024-10-19T06:33:52.487368Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Filename', 'Title', 'Abstract', 'Content', 'Keyword', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 105418\n})"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Convert the Hugging Face dataset to a TensorFlow dataset\ntf_train_dataset = tokenized_train.to_tf_dataset(\n    columns=['input_ids', 'attention_mask'],  # Input features\n    label_cols='labels',                      # Label column\n    shuffle=True,\n    batch_size=16,\n    collate_fn=lambda x: {\n        'input_ids': tf.ragged.constant([i['input_ids'] for i in x], dtype=tf.int32).to_tensor(),\n        'attention_mask': tf.ragged.constant([i['attention_mask'] for i in x], dtype=tf.int32).to_tensor(),\n        'labels': tf.constant([i['labels'] for i in x], dtype=tf.int32)\n    }\n)\n\n# Verify a batch\nfor batch in tf_train_dataset.take(1):\n    inputs, labels = batch\n    print(\"Input IDs shape:\", inputs['input_ids'].shape)\n    print(\"Attention Mask shape:\", inputs['attention_mask'].shape)\n    print(\"Labels shape:\", labels.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T06:36:06.842865Z","iopub.execute_input":"2024-10-19T06:36:06.843688Z","iopub.status.idle":"2024-10-19T06:36:07.796848Z","shell.execute_reply.started":"2024-10-19T06:36:06.843648Z","shell.execute_reply":"2024-10-19T06:36:07.795905Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Input IDs shape: (16, 1024)\nAttention Mask shape: (16, 1024)\nLabels shape: (16, 1024)\n","output_type":"stream"}]},{"cell_type":"code","source":"max_len = 512\n\n# Giảm kích thước dữ liệu trong tập huấn luyện\ntf_train_dataset = tf_train_dataset.map(lambda x, y: ({\n    'input_ids': x['input_ids'][:, :max_len],  # Chỉ lấy 512 token đầu tiên\n    'attention_mask': x['attention_mask'][:, :max_len]\n}, y[:, :max_len]))  # Giảm nhãn về kích thước 512\nmodel.fit(tf_train_dataset.batch(32), epochs=3)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T15:19:02.183425Z","iopub.execute_input":"2024-10-19T15:19:02.183700Z","iopub.status.idle":"2024-10-19T15:19:02.508430Z","shell.execute_reply.started":"2024-10-19T15:19:02.183668Z","shell.execute_reply":"2024-10-19T15:19:02.507287Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Giảm kích thước dữ liệu trong tập huấn luyện\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tf_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf_train_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: ({\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :max_len],  \u001b[38;5;66;03m# Chỉ lấy 512 token đầu tiên\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :max_len]\n\u001b[1;32m      7\u001b[0m }, y[:, :max_len]))  \u001b[38;5;66;03m# Giảm nhãn về kích thước 512\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'tf_train_dataset' is not defined"],"ename":"NameError","evalue":"name 'tf_train_dataset' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Adjust the batch size to match your TensorFlow dataset\nbatch_size = 16\n\n# Calculate steps per epoch and total training steps\nsteps_per_epoch = len(tokenized_train) // batch_size\ntotal_training_steps = steps_per_epoch * epochs\n\n# Create an optimizer using the transformers utility\nfrom transformers import create_optimizer\n\noptimizer, schedule = create_optimizer(\n    init_lr=learning_rate, \n    num_train_steps=total_training_steps, \n    num_warmup_steps=0\n)\n\n# Compile the model with the optimizer and loss function\nmodel.compile(optimizer=optimizer, loss=model.compute_loss)\nmodel.summary()\n\n# Assuming you've created `tf_valid_dataset` in a similar manner as `tf_train_dataset`\n# Train the model\n# model.fit(\n#     tf_train_dataset,\n#     validation_data=tf_valid_dataset,\n#     epochs=epochs\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-19T06:48:18.904532Z","iopub.execute_input":"2024-10-19T06:48:18.905303Z","iopub.status.idle":"2024-10-19T06:48:18.978308Z","shell.execute_reply.started":"2024-10-19T06:48:18.905261Z","shell.execute_reply":"2024-10-19T06:48:18.977449Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Model: \"tfm_bart_for_conditional_generation_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model (TFMBartMainLayer)    multiple                  420361216 \n                                                                 \n final_logits_bias (BiasLay  multiple                  64001     \n er)                                                             \n                                                                 \n=================================================================\nTotal params: 420425217 (1.57 GB)\nTrainable params: 420361216 (1.57 GB)\nNon-trainable params: 64001 (250.00 KB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def summarize_text(texts, max_length=150, min_length=40):\n    \"\"\"\n    Generate summaries for a list of input texts using the trained model.\n    \n    Args:\n    - texts: List of strings containing the texts to summarize.\n    - max_length: Maximum length of the summary.\n    - min_length: Minimum length of the summary.\n    \n    Returns:\n    - List of generated summaries.\n    \"\"\"\n    # Tokenize inputs\n    inputs = tokenizer(texts, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n    \n    # Generate summaries\n    summaries = model.generate(\n        inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_length=max_length,\n        min_length=min_length,\n        length_penalty=2.0,\n        num_beams=4,\n        early_stopping=True\n    )\n    \n    # Decode summaries\n    return [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summaries]\n\n# Example usage:\ninput_texts = [\n    \"Giá vàng sẽ tiếp_tục tăng trong dài hạn, một phần do tình_hình tài_chính bấp_bênh của nhiều quốc_gia phương Tây\"\n]\n\nsummarized_texts = summarize_text(input_texts)\nfor i, summary in enumerate(summarized_texts):\n    print(f\"Summary {i+1}: {summary}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T06:02:41.546850Z","iopub.execute_input":"2024-10-19T06:02:41.547598Z","iopub.status.idle":"2024-10-19T06:02:59.919567Z","shell.execute_reply.started":"2024-10-19T06:02:41.547555Z","shell.execute_reply":"2024-10-19T06:02:59.918546Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Summary 1: Giá vàng sẽ tiếp_tục tăng trong dài hạn, một phần do tình_hình tài_chính bấp_bênh của nhiều quốc_gia phương Tây( * ) : 1 - 1 - 1 - 1 - 1 - 1 - 1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Dropout, LayerNormalization\nfrom tensorflow.keras.models import Model\nfrom transformers import TFBertModel\n\nclass ScaledDotProductAttention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(ScaledDotProductAttention, self).__init__(**kwargs)\n\n    def call(self, query, key, value, mask=None):\n        matmul_qk = tf.matmul(query, key, transpose_b=True)\n        dk = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n        if mask is not None:\n            scaled_attention_logits += (mask * -1e9)\n\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n        output = tf.matmul(attention_weights, value)\n        return output, attention_weights\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n        self.depth = d_model // self.num_heads\n\n        self.wq = Dense(d_model)\n        self.wk = Dense(d_model)\n        self.wv = Dense(d_model)\n\n        self.dense = Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        scaled_attention, _ = ScaledDotProductAttention()(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n\n        output = self.dense(concat_attention)\n        return output\nclass TransformerEncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(TransformerEncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = tf.keras.Sequential([\n            Dense(dff, activation='relu'),\n            Dense(d_model)\n        ])\n\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n        return out2\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T06:46:31.517247Z","iopub.execute_input":"2024-10-19T06:46:31.517934Z","iopub.status.idle":"2024-10-19T06:46:31.617176Z","shell.execute_reply.started":"2024-10-19T06:46:31.517889Z","shell.execute_reply":"2024-10-19T06:46:31.616246Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Thay đổi phần gọi mô hình BERT trong hàm build_model\ndef build_model(vocab_size, max_len, d_model, num_heads, dff):\n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n\n    bert_model = TFBertModel.from_pretrained(\"vinai/phobert-base\")\n\n    # Chuyển đổi Keras Tensors thành TensorFlow tensors\n    input_ids_tensor = tf.convert_to_tensor(input_ids)\n    attention_mask_tensor = tf.convert_to_tensor(attention_mask)\n\n    bert_outputs = bert_model(input_ids_tensor, attention_mask=attention_mask_tensor)\n    x = bert_outputs.last_hidden_state\n\n    encoder_layer = TransformerEncoderLayer(d_model, num_heads, dff)\n    x = encoder_layer(x, training=True, mask=None)\n    x = Dense(vocab_size, activation='softmax')(x)\n\n    model = Model(inputs=[input_ids, attention_mask], outputs=x)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T06:48:53.443859Z","iopub.execute_input":"2024-10-19T06:48:53.444269Z","iopub.status.idle":"2024-10-19T06:48:53.451803Z","shell.execute_reply.started":"2024-10-19T06:48:53.444229Z","shell.execute_reply":"2024-10-19T06:48:53.450775Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"model = build_model(vocab_size=30000, max_len=512, d_model=512, num_heads=8, dff=2048)\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tokenized_train[\"input_ids\"], \n                                                     \"attention_mask\": tokenized_train[\"attention_mask\"]}, \n                                                    tokenized_train[\"labels\"])).batch(32)\n\nmodel.fit(train_dataset, epochs=3)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-19T06:48:57.771087Z","iopub.execute_input":"2024-10-19T06:48:57.771469Z","iopub.status.idle":"2024-10-19T06:48:58.479255Z","shell.execute_reply.started":"2024-10-19T06:48:57.771431Z","shell.execute_reply":"2024-10-19T06:48:58.477876Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\nSome layers from the model checkpoint at vinai/phobert-base were not used when initializing TFBertModel: ['lm_head', 'roberta']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFBertModel were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['bert']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenized_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      5\u001b[0m                                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenized_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, \n\u001b[1;32m      6\u001b[0m                                                     tokenized_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m32\u001b[39m)\n","Cell \u001b[0;32mIn[55], line 9\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(vocab_size, max_len, d_model, num_heads, dff)\u001b[0m\n\u001b[1;32m      6\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m TFBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvinai/phobert-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Chuyển đổi Keras Tensors thành TensorFlow tensors\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m input_ids_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m attention_mask_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(attention_mask)\n\u001b[1;32m     12\u001b[0m bert_outputs \u001b[38;5;241m=\u001b[39m bert_model(input_ids_tensor, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask_tensor)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"],"ename":"ValueError","evalue":"A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n","output_type":"error"}]}]}