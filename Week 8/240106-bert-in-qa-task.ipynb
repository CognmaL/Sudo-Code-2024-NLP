{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4701735,"sourceType":"datasetVersion","datasetId":2627880}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT FOR QUESTION ANSWER TASK","metadata":{}},{"cell_type":"code","source":"# Import nesscessary libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import BertForQuestionAnswering, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport json\nimport os\nfrom transformers import AutoTokenizer, get_linear_schedule_with_warmup\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2024-11-03T05:52:24.074471Z","iopub.execute_input":"2024-11-03T05:52:24.074853Z","iopub.status.idle":"2024-11-03T05:52:24.080557Z","shell.execute_reply.started":"2024-11-03T05:52:24.074818Z","shell.execute_reply":"2024-11-03T05:52:24.079500Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Load data\nwith open('/kaggle/input/vietnamese-squad/dev-v2.0-translated.json', 'r', encoding='utf-8') as f:\n    train_data = json.load(f)\n    \nwith open('/kaggle/input/vietnamese-squad/train-v2.0-translated.json', 'r', encoding='utf-8') as f:\n    dev_data = json.load(f)\n    \n# Format the data for SQuAD-like structure\ndef format_squad_data(data):\n    formatted_data = []\n    for context, question, answer in data:\n        formatted_data.append({\"context\": context, \"question\": question, \"answer\": answer})\n    return formatted_data\n\ntrain_data = format_squad_data(train_data)\ndev_data = format_squad_data(dev_data)\n\nprint(\"Sample data: \\n\", train_data[:2])","metadata":{"execution":{"iopub.status.busy":"2024-11-03T05:10:09.395163Z","iopub.execute_input":"2024-11-03T05:10:09.395723Z","iopub.status.idle":"2024-11-03T05:10:13.311597Z","shell.execute_reply.started":"2024-11-03T05:10:09.395686Z","shell.execute_reply":"2024-11-03T05:10:13.310495Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Sample data: [{'context': 'Người Norman (Norman: Nourmands; French: Normands; Latin: Normanni) là những người trong thế kỷ 10 và 11 đã đặt tên cho Normandy, một khu vực ở Pháp. Họ là hậu duệ của những kẻ cướp biển và cướp biển Norse (\"Norman\" đến từ \"Norseman\") từ Đan Mạch, Iceland và Na Uy, dưới sự lãnh đạo của Rollo, đã đồng ý thề trung thành với Vua Charles III của Tây Francia. Qua nhiều thế hệ đồng hoá và trộn lẫn với dân số Frank và La Mã-Gaulish bản địa, con cháu của họ sẽ dần dần hợp nhất với các nền văn hoá dựa trên Carolingian của Tây Francia. Bản sắc văn hoá và dân tộc riêng biệt của người Norman xuất hiện ban đầu vào nửa đầu thế kỷ 10, và nó tiếp tục phát triển trong các thế kỷ tiếp theo.', 'question': 'Normandy nằm ở quốc gia nào?', 'answer': 'Pháp'}, {'context': 'Người Norman (Norman: Nourmands; French: Normands; Latin: Normanni) là những người trong thế kỷ 10 và 11 đã đặt tên cho Normandy, một khu vực ở Pháp. Họ là hậu duệ của những kẻ cướp biển và cướp biển Norse (\"Norman\" đến từ \"Norseman\") từ Đan Mạch, Iceland và Na Uy, dưới sự lãnh đạo của Rollo, đã đồng ý thề trung thành với Vua Charles III của Tây Francia. Qua nhiều thế hệ đồng hoá và trộn lẫn với dân số Frank và La Mã-Gaulish bản địa, con cháu của họ sẽ dần dần hợp nhất với các nền văn hoá dựa trên Carolingian của Tây Francia. Bản sắc văn hoá và dân tộc riêng biệt của người Norman xuất hiện ban đầu vào nửa đầu thế kỷ 10, và nó tiếp tục phát triển trong các thế kỷ tiếp theo.', 'question': 'Người Norman ở Normandy khi nào?', 'answer': 'Thế kỷ 10 và 11'}]\n","output_type":"stream"}]},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        entry = self.data[idx]\n        \n        # Tokenize với chỉ phần context được cắt ngắn nếu quá dài\n        inputs = self.tokenizer(\n            entry['question'],\n            entry['context'],\n            max_length=self.max_length,\n            truncation=\"only_second\",  # Chỉ cắt ngắn context\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n            return_offsets_mapping=True  # Lấy offset để xác định vị trí câu trả lời\n        )\n\n        input_ids = inputs[\"input_ids\"].squeeze()\n        attention_mask = inputs[\"attention_mask\"].squeeze()\n        offsets = inputs[\"offset_mapping\"].squeeze()\n\n        # Tìm vị trí bắt đầu và kết thúc của câu trả lời trong văn bản gốc\n        answer_start = entry['context'].find(entry['answer'])\n        answer_end = answer_start + len(entry['answer'])\n\n        # Khởi tạo vị trí bắt đầu và kết thúc mặc định nếu không tìm thấy\n        start_positions, end_positions = 0, 0\n\n        # Duyệt qua các token để tìm token chứa vị trí bắt đầu và kết thúc của câu trả lời\n        for i, (start, end) in enumerate(offsets):\n            # Nếu token này bao gồm vị trí bắt đầu của câu trả lời\n            if start <= answer_start < end:\n                start_positions = i\n            # Nếu token này bao gồm vị trí kết thúc của câu trả lời\n            if start < answer_end <= end:\n                end_positions = i\n                break\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"start_positions\": torch.tensor(start_positions),\n            \"end_positions\": torch.tensor(end_positions),\n        }\n\n\n# Load tokenizer and create datasets\ntrain_dataset = QADataset(train_data, tokenizer)\ndev_dataset = QADataset(dev_data, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=8)\n\nfor batch in train_loader:\n    print(\"Input IDs:\", batch['input_ids'])\n    print(\"Attention Mask:\", batch['attention_mask'])\n    print(\"Start Positions:\", batch['start_positions'])\n    print(\"End Positions:\", batch['end_positions'])\n    break \n","metadata":{"execution":{"iopub.status.busy":"2024-11-03T05:56:40.154643Z","iopub.execute_input":"2024-11-03T05:56:40.155006Z","iopub.status.idle":"2024-11-03T05:56:40.264002Z","shell.execute_reply.started":"2024-11-03T05:56:40.154972Z","shell.execute_reply":"2024-11-03T05:56:40.263019Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Input IDs: tensor([[  101, 20646, 16130,  ...,     0,     0,     0],\n        [  101, 11603, 10558,  ...,     0,     0,     0],\n        [  101, 99239, 10116,  ...,     0,     0,     0],\n        ...,\n        [  101, 38500, 18323,  ...,     0,     0,     0],\n        [  101, 53125, 49309,  ...,     0,     0,     0],\n        [  101, 26329, 17031,  ...,     0,     0,     0]])\nAttention Mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]])\nStart Positions: tensor([14,  0,  0, 21, 15, 16, 15, 20])\nEnd Positions: tensor([ 0, 15,  6,  0,  0,  0,  0, 28])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load model\nmodel = BertForQuestionAnswering.from_pretrained(\"bert-base-multilingual-cased\")\n\n# Load tokenizer \ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-03T05:56:44.708731Z","iopub.execute_input":"2024-11-03T05:56:44.709402Z","iopub.status.idle":"2024-11-03T05:56:45.253304Z","shell.execute_reply.started":"2024-11-03T05:56:44.709362Z","shell.execute_reply":"2024-11-03T05:56:45.252490Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Ý nghĩa warning notification: \n\n- Mô hình bert-base-uncased là một mô hình BERT gốc được huấn luyện cho các tác vụ chung chung. Tuy nhiên, khi sử dụng mô hình này cho tác vụ trả lời câu hỏi (Question Answering), một số lớp cuối của mô hình (`qa_outputs`) không có sẵn trong checkpoint và cần được thêm vào.\n\n- Cho nên, các tham số này (`qa_outputs.bias`, `qa_outputs.weight`) cần được khởi tạo ngẫu nhiên.\n\nĐể giải quyết cảnh báo này, có thể thực hiện huấn luyện lại các tham số của mô hình cho tác vụ trả lời câu hỏi.","metadata":{}},{"cell_type":"code","source":"# Load the BERT model for QA and move it to the device (CPU/GPU)\nmodel.to(device)\nprint(f\"Successfully moved model to {device}!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-03T05:56:49.128512Z","iopub.execute_input":"2024-11-03T05:56:49.128868Z","iopub.status.idle":"2024-11-03T05:56:49.376670Z","shell.execute_reply.started":"2024-11-03T05:56:49.128835Z","shell.execute_reply":"2024-11-03T05:56:49.375752Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Using device: cuda\nSuccessfully moved model to device!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Hyperparameters\nlearning_rate = 2e-5     # Giá trị này thường nằm trong khoảng từ 1e-5 đến 5e-5 cho các mô hình BERT.\nbatch_size = 8           # Giá trị này nên được chọn dựa trên bộ nhớ GPU có sẵn và kích thước của dữ liệu.\nnum_epochs = 2           # Số lượng epoch nên được chọn dựa trên độ phức tạp của mô hình và kích thước của tập dữ liệu.\nweight_decay = 0.01      # Giá trị này thường nằm trong khoảng từ 0 đến 0.1, với giá trị phổ biến là 0.01\nmax_length = 384         # Giá trị thường được chọn cho max_length là từ 384 đến 512. \ndoc_stride = 128         # Giá trị thường chọn 128 tokens hoặc xuống 64 nếu thấy rằng mô missing nhiều thông tin quan trọng\nwarmup_steps_ratio = 0.1 # Tỷ lệ từ 0.1 đến 0.2 thường được khuyến nghị","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Định nghĩa:**\n\n- `learning_rate`:  Tốc độ học của mô hình. Tốc độ học quá cao có thể làm mô hình không hội tụ, trong khi tốc độ quá thấp có thể làm cho quá trình huấn luyện diễn ra chậm hơn.\n- `batch_size`: Số lượng mẫu trong mỗi lô (batch) khi huấn luyện. Một batch_size lớn hơn có thể giúp tăng tốc quá trình huấn luyện nhưng yêu cầu nhiều bộ nhớ GPU hơn.\n- `num_epochs`: Số lượng lần mô hình sẽ duyệt qua toàn bộ tập huấn luyện. Nếu số epoch quá thấp, mô hình có thể chưa học đủ; nếu quá cao, có thể xảy ra hiện tượng quá khớp (overfitting).\n- `weight_decay`: Tham số giúp giảm thiểu hiện tượng quá khớp bằng cách thêm một hình phạt cho các trọng số lớn trong quá trình tối ưu hóa. Nó có thể điều chỉnh tốt độ chính xác mà không làm giảm khả năng học của mô hình.\n- `max_length`: Độ dài tối đa cho đầu vào (câu hỏi + ngữ cảnh). Giới hạn này giúp kiểm soát việc cắt bớt (truncation) dữ liệu quá dài.\n- `doc_stride`: Khoảng cách trượt giữa các đoạn ngữ cảnh, cho phép mô hình xử lý ngữ cảnh dài hơn mà không bỏ lỡ thông tin quan trọng.\n- `warmup_steps_ratio`: Tỉ lệ bước khởi động để làm mượt tốc độ học trong giai đoạn đầu, tránh các biến động lớn trong quá trình huấn luyện.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Define optimizer\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\ntotal_steps = len(train_loader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * warmup_steps_ratio), num_training_steps=total_steps)\n\n# Training loop\ndef train(model, dataloader, optimizer, scheduler):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n\n        # Compute loss\n        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n    avg_loss = total_loss / len(dataloader)\n    return avg_loss\n\nfor epoch in range(num_epochs):\n    avg_loss = train(model, train_loader, optimizer, scheduler)\n    print(f\"Epoch {epoch+1}/{num_epochs} completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-11-03T05:57:47.077277Z","iopub.execute_input":"2024-11-03T05:57:47.077665Z","iopub.status.idle":"2024-11-03T06:39:51.355095Z","shell.execute_reply.started":"2024-11-03T05:57:47.077631Z","shell.execute_reply":"2024-11-03T06:39:51.354145Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/2 completed\nEpoch 2/2 completed\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef evaluate(model, dataloader):\n    model.eval()\n    exact_match = 0\n    total = 0\n    \n    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        \n        with torch.no_grad():\n            outputs = model(input_ids, attention_mask=attention_mask)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n\n        # Chọn vị trí bắt đầu và kết thúc với xác suất cao nhất\n        start_preds = torch.argmax(start_logits, dim=1)\n        end_preds = torch.argmax(end_logits, dim=1)\n\n        # Kiểm tra tính chính xác\n        for i in range(len(start_positions)):\n            total += 1\n            if start_preds[i] == start_positions[i] and end_preds[i] == end_positions[i]:\n                exact_match += 1\n    \n    accuracy = exact_match / total if total > 0 else 0\n    print(f\"Accuracy: {accuracy:.4f}\")\n    return accuracy\n\naccuracy = evaluate(model, dev_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-03T06:59:57.082062Z","iopub.execute_input":"2024-11-03T06:59:57.082803Z","iopub.status.idle":"2024-11-03T08:16:35.979158Z","shell.execute_reply.started":"2024-11-03T06:59:57.082762Z","shell.execute_reply":"2024-11-03T08:16:35.978253Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 16290/16290 [1:16:38<00:00,  3.54it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.2920\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(\"./fine_tuned_bert_qa\")\ntokenizer.save_pretrained(\"./tokenizer_fine_tuned_bert_qa\")","metadata":{"execution":{"iopub.status.busy":"2024-11-03T08:20:06.470206Z","iopub.execute_input":"2024-11-03T08:20:06.471003Z","iopub.status.idle":"2024-11-03T08:20:08.373382Z","shell.execute_reply.started":"2024-11-03T08:20:06.470962Z","shell.execute_reply":"2024-11-03T08:20:08.372470Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"('./tokenizer_fine_tuned_bert_qa/tokenizer_config.json',\n './tokenizer_fine_tuned_bert_qa/special_tokens_map.json',\n './tokenizer_fine_tuned_bert_qa/vocab.txt',\n './tokenizer_fine_tuned_bert_qa/added_tokens.json',\n './tokenizer_fine_tuned_bert_qa/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"def inference(model, question, context, tokenizer, max_length=384, doc_stride=128):\n    model.eval()\n    \n    # Tokenize câu hỏi và ngữ cảnh\n    inputs = tokenizer(\n        question,\n        context,\n        max_length=max_length,\n        truncation=\"only_second\",\n        padding=\"max_length\",\n        stride=doc_stride,\n        return_tensors=\"pt\"\n    ).to(device)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n\n    # Tính toán vị trí bắt đầu và kết thúc có xác suất cao nhất\n    start_index = torch.argmax(start_logits)\n    end_index = torch.argmax(end_logits)\n\n    # Chuyển đổi các token đã mã hóa thành chuỗi văn bản\n    all_tokens = inputs['input_ids'].squeeze().tolist()\n    answer_tokens = all_tokens[start_index:end_index + 1]\n    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-11-03T08:20:19.463509Z","iopub.execute_input":"2024-11-03T08:20:19.464128Z","iopub.status.idle":"2024-11-03T08:20:19.472189Z","shell.execute_reply.started":"2024-11-03T08:20:19.464081Z","shell.execute_reply":"2024-11-03T08:20:19.471258Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Q & A:\nquestion = \"Nước biển ở đâu có hàm lượng muối thấp nhất?\"\ncontext = \"Nước biển có độ mặn không đồng đều trên toàn thế giới mặc dù phần lớn có độ mặn nằm trong khoảng từ 3,1% tới 3,8%. Khi sự pha trộn với nước ngọt đổ ra từ các con sông hay gần các sông băng đang tan chảy thì nước biển nhạt hơn một cách đáng kể. Nước biển nhạt nhất có tại vịnh Phần Lan, một phần của biển Baltic.\"\nanswer = inference(model, question, context, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T08:20:59.722810Z","iopub.execute_input":"2024-11-03T08:20:59.723640Z","iopub.status.idle":"2024-11-03T08:20:59.766070Z","shell.execute_reply.started":"2024-11-03T08:20:59.723600Z","shell.execute_reply":"2024-11-03T08:20:59.765267Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Question: Nước biển ở đâu có hàm lượng muối thấp nhất?\nAnswer: \n","output_type":"stream"}]},{"cell_type":"markdown","source":"---------","metadata":{}}]}